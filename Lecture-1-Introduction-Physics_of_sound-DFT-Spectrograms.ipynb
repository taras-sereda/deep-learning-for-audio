{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for audio Lecture 1 (Introduction)\n",
    "\n",
    "\n",
    "Usefull links:\n",
    "\n",
    "1. Great web based plotting tool: [Desmos](https://www.desmos.com/calculator)\n",
    "2. Video explanation of [Fourier tranform](https://www.youtube.com/watch?v=spUNpyF58BY)(3blue1brown)\n",
    "5. Introductory tutorials on [FFT/MFCC/VAD](http://practicalcryptography.com/miscellaneous/machine-learning/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics of sound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sound is a wave or oscilation represented by air preasure disturbance cuased by vibration.\n",
    "- Sound is a movement of molecules in the air \n",
    "\n",
    "Preconditions for a sound hearable by a human:\n",
    " - source of vibration: guitar string, human voice, antyhing that vibrates in a hearable range of frequencies (20 - 20k Hz)\n",
    " - elastic medium: air mulecules, watter.\n",
    "\n",
    "[Interactive intorduction to waveforms](https://pudding.cool/2018/02/waveforms/)\n",
    "\n",
    "Waveform properties:\n",
    " - frequency - number of full cycles per 1 second\n",
    " - amplitude - roughly equivalent to loudness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "    <script src=\"https://www.desmos.com/api/v1.6/calculator.js?apiKey=dcb31709b452b1cf9dc26972add0fda6\"></script>\n",
    "  </head>\n",
    "  <body>\n",
    "    <div id=\"calculator\" style=\"width: 100%; height: 600px;\"></div>\n",
    "    <script>\n",
    "      var elt = document.getElementById('calculator');\n",
    "      var calculator = Desmos.GraphingCalculator(elt);\n",
    "      // Set the expression to plot y = sin(x)\n",
    "      calculator.setExpression({ id: 'graph1', latex: 'y=\\\\\\\\sin(203x) + \\\\\\\\sin(3x)  + 2\\\\\\\\sin(50x) + \\\\\\\\cos(2000x)'});\n",
    "    </script>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Audio representation\n",
    "![image.png](./assets/Signal_Sampling.svg.png)\n",
    "\n",
    "[Sampling](https://en.wikipedia.org/wiki/Sampling_(signal_processing)#Applications) - take a continues wave and discritize it. \n",
    "\n",
    "**Sampling rate** - density of discretization. Common values (8000 Hz, 16000 Hz, 22050 Hz, 44100 Hz)\n",
    "- 44100 Hz - can represent 20 kHz, maximum audible frequency by humans.\n",
    "- 22050 Hz - Speech Synthesis models. was used for low bit rate MP3 in the past.\n",
    "- 16000 Hz - widely used for training ASR models. Can represent human speech frequency spectrum (200 Hz - 8 kHz)\n",
    "- 8000 Hz - telephone, encrypted walkie-talkie, adequate for human speech, but without fricative sounds /s/ /f/\n",
    "\n",
    "\n",
    "**Bit depth** - resolution of each sample. 16 bit, 8 bit. etc.\n",
    "\n",
    "$$ bit\\_rate = sampling\\_rate \\times bit\\_depth $$\n",
    "\n",
    "[Nyquist-Shannon sampling theoreme](https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem) bridges continues and discrete signals\n",
    "\n",
    "\n",
    "In short ucompressed PCM format represents sampled amplited at specified sampling rate and bitdepth. Please reffer to [ffmpeg wiki](http://trac.ffmpeg.org/wiki/audio%20types) on more detials about audio formats. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SoX - Sound eXchange\n",
    "\n",
    "\n",
    "The Swiss Army knife of audio manipulation, like ffmpeg for video.\n",
    "\n",
    "\n",
    "- query usefull audio properties via __soxi__\n",
    "\n",
    "```\n",
    "(venv)$ soxi LJ037-0171.wav\n",
    "\n",
    "Input File     : 'LJ037-0171.wav'\n",
    "Channels       : 1\n",
    "Sample Rate    : 22050\n",
    "Precision      : 16-bit\n",
    "Duration       : 00:00:07.58 = 167226 samples ~ 568.796 CDDA sectors\n",
    "File Size      : 334k\n",
    "Bit Rate       : 353k\n",
    "Sample Encoding: 16-bit Signed Integer PCM\n",
    "```\n",
    "\n",
    "- resample audio\n",
    "\n",
    "```\n",
    "(venv)$ sox LJ037-0171.wav LJ037-0171.8k.wav rate 8000\n",
    "(venv)$ soxi LJ037-0171.8k.wav\n",
    "\n",
    "Input File     : 'LJ037-0171.8k.wav'\n",
    "Channels       : 1\n",
    "Sample Rate    : 8000\n",
    "Precision      : 16-bit\n",
    "Duration       : 00:00:07.58 = 60672 samples ~ 568.8 CDDA sectors\n",
    "File Size      : 121k\n",
    "Bit Rate       : 128k\n",
    "Sample Encoding: 16-bit Signed Integer PCM\n",
    "```\n",
    "\n",
    "- play audio directly in terminal!\n",
    "\n",
    "```\n",
    "(venv)$ play LJ037-0171.wav\n",
    "\n",
    "LJ037-0171.wav:\n",
    "\n",
    " File Size: 334k      Bit Rate: 353k\n",
    "  Encoding: Signed PCM\n",
    "  Channels: 1 @ 16-bit\n",
    "Samplerate: 22050Hz\n",
    "Replaygain: off\n",
    "  Duration: 00:00:07.58\n",
    "\n",
    "In:58.8% 00:00:04.46 [00:00:03.13] Out:168k  [  -===|===-  ]        Clip:0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other usefull audio processing tools and libraries\n",
    "\n",
    "- [Audacity](https://www.audacityteam.org/) opensource GUI for audio recording,editting, convenient manipulations.\n",
    "- [Librosa](https://librosa.org/doc/latest/index.html) convenient python library for audio feature extraction, manipulation, and builidng ML pipelines. Gradually looses it's importance, perpahs due to emergence of more end-to-end models.\n",
    "- [PyTorch Audio](https://github.com/pytorch/audio) seamless integration with PyTorch, under active development, still not mature but promissing library.\n",
    "- [FFmpeg](https://ffmpeg.org/) Indispensible tool when you work with audio and video. Allows to quickly resample audio files, trim silence, apply effects, normalize loudness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Spectral Representations\n",
    "\n",
    "- Spectrogram(STFT): $ S_{stft} \\in \\mathbb{C}^{L \\times F} $, where $F$ - number of frequencies\n",
    "- Mel-spectrogram: $ S_{mel} \\in  \\mathbb{R}^{L \\times M} $, where $M$ - number of MEL filters\n",
    "- Gammatone: $ S_{gamma} \\in \\mathbb{R}^{L \\times G} $, where $G$ - number of gammatone filters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete Fourier Transform\n",
    "\n",
    "$$ {\\huge X_k = \\sum_{n=0}^{N-1} x_n \\cdot e ^ {\\frac{-i2\\pi}{N}kn} } $$ \n",
    "\n",
    "Key Idea! Represent time domain waveform through new basis of trig functions, ($\\cos and \\sin$) of various frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dft(x):\n",
    "    # Naive implementation of Discrete Fourier transform (DFT)\n",
    "    # https://en.wikipedia.org/wiki/Discrete_Fourier_transform\n",
    "    assert x.ndim == 1\n",
    "    N = x.shape[0]\n",
    "    K = x.shape[0]\n",
    "    spectrum = np.zeros_like(x, dtype='complex')\n",
    "    for i in range(K):\n",
    "        for j in range(N):\n",
    "            spectrum[i] += x[j] * np.exp(-1j * 2 * np.pi * i * j / N)\n",
    "            \n",
    "    return spectrum\n",
    "\n",
    "def get_magnitude(x):\n",
    "    # |x| = sqrt(a^2 + b^2)\n",
    "    assert x.dtype == 'complex'\n",
    "    return np.sqrt(x.real**2 + x.imag**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = 'data/wav_22050/piano.22050.wav'\n",
    "audio_waveform, sampling_rate = librosa.core.load(audio_path, sr=None, duration=5.0, offset=5.0)\n",
    "audio_waveform = audio_waveform[:sampling_rate * 5]\n",
    "\n",
    "librosa.display.waveshow(audio_waveform, sr=sampling_rate)\n",
    "display(Audio(data=audio_waveform, rate=sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum = get_magnitude(dft(audio_waveform[-512:]))\n",
    "spectrum_2 = np.abs((np.fft.fft(audio_waveform[-512:])))\n",
    "\n",
    "assert np.allclose(spectrum, spectrum_2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 4))\n",
    "axes[0].plot(spectrum)\n",
    "axes[0].set_title('redundant DFT')\n",
    "axes[1].plot(spectrum[:256+1])\n",
    "axes[1].set_title('onesided DFT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectrogram, ShortTimeFourierTransform (STFT)\n",
    "\n",
    "- Key Idea, slide over waveform with a window and compute DFT, concatenate DFT's for each window in a single 2D Matrix.\n",
    "- has a well defined inverse transformation iSTFT\n",
    "\n",
    "![stft_viz](./assets/stft_output.png)\n",
    "image taken from [MathWorks](https://www.mathworks.com/help/dsp/ref/dsp.stft.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_stft = librosa.stft(audio_waveform)\n",
    "librosa.display.specshow(librosa.amplitude_to_db(np.abs(audio_stft), ref=np.max), y_axis='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mel-Spectrogram\n",
    "\n",
    "- Perceptual scale of pitches equal in distance from one another.\n",
    "- Mel comes from melody, to indicate that name comes from pitch comparison.\n",
    "- Mel-spectrogram is obtained from stft by one more matrix multiplication with mel-filterbank\n",
    "- Intuitively shows that humans distiguish low frequencies better then high frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 20\n",
    "mel_basis = librosa.filters.mel(sr=sampling_rate, n_fft=2048, n_mels=n_mels)\n",
    "for i in range(n_mels):\n",
    "    plt.plot(mel_basis[i, :])\n",
    "plt.title('Mel Basis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gammatones. Slaney, 1998\n",
    "\n",
    "- Spectrogram like represnetations based on modelling how the human ear perceives, emphasises and separates different frequencies of sound.\n",
    "- Gammatone should represent the human experience of sound better than, say, a Fourier-domain spectrum.\n",
    "- Approximate version of Gammatones can be computed from Spectrogramms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gammatone import gtgram\n",
    "# https://github.com/detly/gammatone\n",
    "# Scipy implelementation of gammatone filters\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.gammatone.html\n",
    "def compute_gammatone(wav):\n",
    "    window_time = 0.02\n",
    "    hop_time = window_time / 2\n",
    "    channels = 120\n",
    "    f_min = 20\n",
    "    gtg = gtgram.gtgram(wav, sampling_rate, window_time, hop_time, channels, f_min)\n",
    "    gtg = np.flipud(20 * np.log10(gtg))\n",
    "    return gtg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_dir = pathlib.Path('./data/wav_22050/')\n",
    "for wav_path in wav_dir.glob('*.wav'):\n",
    "    print(f'{wav_path.name}')\n",
    "    wav, sampling_rate = librosa.core.load(wav_path, sr=None, duration=5.0)\n",
    "    stft = librosa.stft(wav)\n",
    "    gamma = compute_gammatone(wav)\n",
    "    mel = librosa.feature.melspectrogram(y=wav, sr=sampling_rate)\n",
    "    \n",
    "    display(Audio(data=wav, rate=sampling_rate))\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(12,14))\n",
    "    fig.tight_layout(pad=3.0)\n",
    "\n",
    "    librosa.display.waveshow(wav,sr=sampling_rate, ax=axes[0])\n",
    "    axes[0].set_title('waveform')\n",
    "\n",
    "    librosa.display.specshow(librosa.amplitude_to_db(np.abs(stft), ref=np.max),ax=axes[1])\n",
    "    axes[1].set_title('STFT-spectrogram')\n",
    "\n",
    "    librosa.display.specshow(librosa.amplitude_to_db(mel, ref=np.max), ax=axes[2])\n",
    "    axes[2].set_title('MEL-spectrogram')\n",
    "\n",
    "    axes[3].imshow(gamma)\n",
    "    axes[3].set_title('gammatone')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "## Multi-purpose\n",
    "\n",
    "[Audioset](https://research.google.com/audioset/index.html) \n",
    "- videos from YouTube. Useful for many purposes, audio classification, urban sounds recognition, noises for augmentations.\n",
    "- 2.1 million annotated videos\n",
    "- 5.8 thousand hours of audio\n",
    "- 527 classes of annotated sounds\n",
    "\n",
    "[LAION-audio-dataset](https://github.com/LAION-AI/audio-dataset)\n",
    "- is an umbrella collection of multiple dataset of thousands of hours.\n",
    "- useful for training unsupervised/self-supervised models.\n",
    "\n",
    "## English TTS/STT data\n",
    "[LJSpeech](https://keithito.com/LJ-Speech-Dataset/) \n",
    "- very clean and high quality.\n",
    "- 24 hours, 13,100 short audio clips.\n",
    "- single speaker reading passages from 7 non-fiction books.\n",
    "\n",
    "[GIGA Speech](https://github.com/SpeechColab/GigaSpeech)\n",
    "- large scale 10k hours dataset.\n",
    "- contains podcasts/youtube videos/audio books.\n",
    "\n",
    "## Multilingual data suitable TTS and STT\n",
    "[Common Voice](https://voice.mozilla.org/en)\n",
    "- huge dataset with more than 100 languages and thousands of speakers.\n",
    "- has demographic metadata like age, sex, and accent.\n",
    "  \n",
    "[MAI-LABS](https://www.caito.de/2019/01/03/the-m-ailabs-speech-dataset/)\n",
    "- total amount 1000 hours, in 9 languages.\n",
    "- ukrainian data - audiobooks with total duration of 87 hours."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-env",
   "language": "python",
   "name": "new-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
